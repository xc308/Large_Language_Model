{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFMTtbiQPWbWbPxdG7h5br",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xc308/Large_Language_Model/blob/main/3_Loading_Models_and_Inference_with_Hugging_Face_Inferences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsmM1fE7kFaF",
        "outputId": "ffe2aaa0-e591-4831-e736-03f7225caa98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m794.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers # for accessing pretrained models and performing various NLP tasks with ease"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# suppress warnings:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "kjvVoPhSlO27"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text classification with DistilBERT\n"
      ],
      "metadata": {
        "id": "qnu84GJVlzax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the model and tokenizer\n",
        "initialize a tokenizer and a model for sentiment analysis using DistilBERT fine-tuned on the SST-2 dataset"
      ],
      "metadata": {
        "id": "ghDDr171mTAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "4962a773ec8c4e0d88d072fceea099bc",
            "41200b63402c4eee844021a09cf41f3f",
            "d6e545af3fb04ffa9fef0a842513377e",
            "fcf735b20a45425f87e19c13e80990f0",
            "0d097d6bff9e43a7915f78b770de44f0",
            "81dc561941d542e3afbac4a95f6ff696",
            "f14bbc6ba5a04d779af23b931427af24",
            "b0b986693e5a43aba010847937c41b8d",
            "c605784365d04446b44c3d6a5be26db4",
            "24f01fad116a43958f4d02b7a0cc4e9d",
            "03e1aeb817c7467299a7da907891a8be",
            "6863214cd8744c3bb9b05b967db45747",
            "b57651144705490eae2fadf0bab6ebc8",
            "7052f1d53b474109b4d767bba29da9cf",
            "efc902cbf1fa4ee381cdcdc0a5387772",
            "1008a1474714407e8c4e0b92138fa927",
            "03ffff23d886466599ea43d4dac947c0",
            "86f77f3ec9e847aaa917908b57bbc73a",
            "43acdcd20b594080bd6b2b75f30a98d6",
            "e8c63808060a43d2b7199820da72ac88",
            "0a08c346cf2a40fa9c1fcd727a1aadca",
            "d95678acb85940d69889a317b7e1805b",
            "d78ba2683c08479e8cad503842f9ab07",
            "b3b40d56c4534b9a81c2c65d6e4d938a",
            "87c29c56abdd41629dcafff80d4e4047",
            "4c3ab005ca7b4d22b94475b4640b81e8",
            "f409ab0cc3ea4d49ad8e8de60e78fb2a",
            "ba7923f8420948f3aa7e4f6ae77f2818",
            "4c42982fbca444069b26e84b3c704683",
            "8b617a0a649848988ddfb3dbcbdddbd3",
            "0e6a1b76db9844ac8a89ec357f5772bb",
            "d00b5d1db6c5401395aa9492ba03d7b5",
            "edb182b74d174a2799942f65574c7bfb",
            "bea76746cb6c406aa0e9b7292e1e4d68",
            "940d57403f3f4d0ca70f3cc23a2335c5",
            "2da45c0bcc584f7c9ad24b8bb26063ca",
            "9f9b95ed7f0749bb9f2dee9e2c53de44",
            "31c5f5e012804a669b961d59a01f6045",
            "70bd95682c2b4b40bc299f84c421d8d2",
            "3309cbf496014c7b84385b32705643d6",
            "671bb77a3db04794b1a71487ce786378",
            "1dc6e0327772445ba408ed8abced8d87",
            "7994d99984e44dc98e80efdc6be4b488",
            "beeeff8627be43bdac73fa36c19837a2"
          ]
        },
        "id": "XIB6TfB1l007",
        "outputId": "4bf095ce-040b-4256-e4ae-7c03a4617853"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4962a773ec8c4e0d88d072fceea099bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6863214cd8744c3bb9b05b967db45747"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d78ba2683c08479e8cad503842f9ab07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bea76746cb6c406aa0e9b7292e1e4d68"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocess the input text\n",
        "tokenize the text"
      ],
      "metadata": {
        "id": "Cv31Jd-bmfs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJUmROvSmcAZ",
        "outputId": "69c348ff-e94d-4300-edc2-569796e62641"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101, 23156,   999,  2017,  1005,  2310,  2180,  1037,  2489,  7281,\n",
            "          2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perform inference"
      ],
      "metadata": {
        "id": "Hcqqj6aDnIAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inference\n",
        "with torch.no_grad(): # not training the model, so no need to compute and save grad, to save memory\n",
        "    outputs = model(**inputs) # ** unpack the inputs"
      ],
      "metadata": {
        "id": "m7D_4MqonAdx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get the logits\n",
        "logits are raw, unnormalized predictions of the model"
      ],
      "metadata": {
        "id": "wZjwO4ixnhlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = outputs.logits\n",
        "logits.shape\n",
        "\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKyMStodnc5v",
        "outputId": "61e76de8-c4e9-4548-ef87-c2987c79b0f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-3.9954,  4.3336]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Post-process the output\n",
        "Convert the logits to probabilities and get the predicted class:"
      ],
      "metadata": {
        "id": "3clhxwOOnzZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert logits to probabilities\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_class = torch.argmax(probs, dim=-1)\n",
        "\n",
        "# Map the predicted class to the label\n",
        "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
        "predicted_label = labels[predicted_class]\n",
        "\n",
        "\n",
        "print(f\"Predicted label: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TK6MQOXnsra",
        "outputId": "16f75a57-123e-4fad-a2ee-3d3d1d1a2cda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: POSITIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text generation with GPT-2\n",
        "\n",
        "##Load tokenizer"
      ],
      "metadata": {
        "id": "zH-1Tf4ApSyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296,
          "referenced_widgets": [
            "0093def52433404990f5f6fb8e54a114",
            "3efb24011ca04bf09f2326373fd90ba7",
            "92fb0681aaa6401087e6d6ca2918be89",
            "df6c8a15847542cfb6d131041f9dbe7e",
            "3a0034c1b3e54858bd06bede15fdf4b7",
            "1305153e411f483389e8345cec7ac74b",
            "c36c8a5e6c674f51ba5d6c6ba9fb7eb3",
            "01d0564b9cd04c14b187368c1e57cbdf",
            "9883280ea9af423ea2f49bc59b802c38",
            "f081631a6a904ce890bef3fe8d988824",
            "b44b84dd82ad4d978a58a198632207ac",
            "d8bd62f31df2432e97146b66dfbf908a",
            "3bb3fdca7fc44e7b851e53f295a2c726",
            "903d22556d514432a241d501bb1f0bed",
            "98a40d51d6e2413e90f617c8250faf86",
            "9316f64e6bbe4b3baeb7333db9b6ee85",
            "2545797d98b243e39d7872ef33c18aa1",
            "5f359841d2b9414dbdbc76eab1b4beaa",
            "b06d7b9ff3884b44bcdcbd5a5fd9998d",
            "2512cd6445bf417faef6ae1b90dd177d",
            "5c153380cb0048dab18b13731217310e",
            "fec567407a554e9da87023c1837a1525",
            "2f2aca503e3b4bfab32c5fd9519b79ff",
            "c364d3ff5d9d495ea0f71d1fe71e8002",
            "72ed65ce10554c8e888b7faa45aa735a",
            "77a80d51e4814a6999f051fa544682c6",
            "627d7ee07bd4449fbfae96263b358626",
            "a0c5acd2d9044dd1934914055825290b",
            "d724e34d4c1245519c3ae6b87504a312",
            "9ad65d1929ff495d819929da92980283",
            "2b7785c4dd7e433db4e132a1cd9b9dad",
            "043fdaf405d8456cae531b772acf9f4b",
            "072b9df926a143c4986cfb3d38c11c1c",
            "741c1659392b47648514dc58eab2e5b7",
            "329de576b5b74fcc8a8d0eb249219feb",
            "fd7535c6507f45f9982b76dd524db788",
            "4b898dfc7cc545888e442f19445d6b23",
            "5080c278c45f423391d3802da54a1efe",
            "cc2b8ddcb074440b8846f5687644c113",
            "014fc1e8711640229189cb0d66adce56",
            "f166997b16b24b158bda0e5e399fac8c",
            "17c5eb79b43a454993847657823524aa",
            "5b425da5f6f24178a06522393e44ede6",
            "59c476be9730417ba7404ffddbc7dbb7",
            "b1494eb8d93c4b72934c6d5b47d28c84",
            "6d516678c8b54df7921edebee1851591",
            "971fc3fa274b4aee903f471b83761279",
            "baacda3c5f5e4e9e84b7aa6a1e38d05d",
            "a0e38cb81af045b4ba5ea0fd715f914e",
            "4f0780e9c5414134b2e06d705830fec0",
            "cdd7837dbfd243af94a644eec5ea05d6",
            "d4fa03bd80614f5aa6895e8a05bbf9a0",
            "9fcfb70ad58d4295860a8cd0780436d0",
            "837b8e0d3f24492d9488ddce2c4e108c",
            "61ab9e3576934b8cb9c40c982abeb71f",
            "b56e7c01f3d348919d4c3c9d73291b71",
            "1f1aa43e77084c99a0c1905bb0893c0c",
            "68cedc6e07664db299e4212422001808",
            "445b2b6cbdd44a199bf70536079273d3",
            "aa85771d18004285876733d836b57ead",
            "a4f810c113e54f9d9749ff8f74197527",
            "bda2b9ebcb1c430eb2f9e556199e44bf",
            "882d96f1101b478d9341d430c4dd14cd",
            "7bf4d533e7d949d7a20dbc03cf3f34cc",
            "54289d0cd7924289a3985c0e872cb057",
            "1eba7033e69146c9b0aff8eaae7c1c8a",
            "120fbf821a34486d9d348ea35642af49",
            "2c1c20405a834b5ca019f71852bc4ff0",
            "451446579ee44c0fb7dd01374e269140",
            "324927b003834fac80c56beb2eebf82f",
            "c657849298694911b075c16aefc72c2a",
            "e7c6cff82e53414cb298e98e9a28eac7",
            "872aabac04a5430eab5be992b1f8d7c8",
            "8995f83bee9446e9a6fd08b69f0f7bcf",
            "6a7bcb1e6f60412cb01bbae4dbe2a355",
            "10679376fb7943ab9d16220f9913c93a",
            "61c056a5956149f9a99f9a4d31ee341a"
          ]
        },
        "id": "OoI5qi1HpQPl",
        "outputId": "7cf7ce7b-38fc-4788-a910-9761d5d2d5b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0093def52433404990f5f6fb8e54a114"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8bd62f31df2432e97146b66dfbf908a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f2aca503e3b4bfab32c5fd9519b79ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "741c1659392b47648514dc58eab2e5b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1494eb8d93c4b72934c6d5b47d28c84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b56e7c01f3d348919d4c3c9d73291b71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "120fbf821a34486d9d348ea35642af49"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess the input text"
      ],
      "metadata": {
        "id": "IZzA80woprxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt = \"Once upon a time\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO_zktZ1pn7G",
        "outputId": "d33e1cb6-917d-4662-fce4-389d384727c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perform inference"
      ],
      "metadata": {
        "id": "8CiVnOWpqAZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "output_ids = model.generate(\n",
        "    inputs.input_ids,  #Input token IDs from the tokenizer\n",
        "    attention_mask=inputs.attention_mask,  # Mask indicating which tokens to attend to\n",
        "    pad_token_id=tokenizer.eos_token_id,   # Padding token ID set to the end-of-sequence token ID\n",
        "    max_length=50,                         # Maximum length of the generated sequences\n",
        "    num_return_sequences=1                 # Number of sequences to generate\n",
        ")\n",
        "\n",
        "output_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_eS1Vxyp9_U",
        "outputId": "4b7d4dad-b472-4e9e-88e3-02aa043df238"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,\n",
              "         8737,  290, 1049, 3514,   13,  383,  995,  373,  257, 1295,  286, 1049,\n",
              "         3514,   11,  290,  262,  995,  373,  257, 1295,  286, 1049, 3514,   13,\n",
              "          383,  995,  373,  257, 1295,  286, 1049, 3514,   11,  290,  262,  995,\n",
              "          373,  257]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-process the output\n",
        "\n",
        "Decode the generated tokens to get the text:"
      ],
      "metadata": {
        "id": "3HYwo1UmqoRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY_Quk3oqk2r",
        "outputId": "3458db36-07e5-4844-f0c9-5f27d9811d6a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hugging Face pipeline() function\n",
        "\n",
        "- pipeline() function from the Hugging Face transformers library\n",
        "- It abstracts the complexities of model loading, tokenization, inference, and post-processing, allowing users to perform complex NLP tasks with just a few lines of code."
      ],
      "metadata": {
        "id": "Yw9Oy1Psr2T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Definition\n",
        "\n",
        "transformers.pipeline(\n",
        "    task: str,                     # The task to perform, such as \"text-classification\", \"text-generation\", \"question-answering\"\n",
        "    model: Optional = None,        # an be a string (model identifier from Hugging Face model hub), a path to a directory containing model files, or a pre-loaded model instance\n",
        "    config: Optional = None,       # The configuration to use. This can be a string, a path to a directory, or a pre-loaded config object.\n",
        "    tokenizer: Optional = None,    # The tokenizer to use. This can be a string, a path to a directory, or a pre-loaded tokenizer instance.\n",
        "    feature_extractor: Optional = None,  # he feature extractor to use for tasks that require it (e.g., image processing).\n",
        "    framework: Optional = None,          # The framework to use, either \"pt\" for PyTorch\n",
        "    revision: str = 'main',              # The specific model version to use\n",
        "    use_fast: bool = True,               # Whether to use the fast version of the tokenizer if available\n",
        "    model_kwargs: Dict[str, Any] = None, # Additional keyword arguments passed to the model during initialization.\n",
        "    **kwargs                             # Additional keyword arguments passed to the pipeline components\n",
        ")"
      ],
      "metadata": {
        "id": "vIAPPJDlr3Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text classification using pipeline()\n",
        "\n",
        "load a pretrained text classification model and use it to classify a sample text."
      ],
      "metadata": {
        "id": "mksRF-qGu5mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a general text classification model\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Classify a sample text\n",
        "result = classifier(\"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\")\n",
        "print(result) #The confidence score for the prediction."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2hJGh6LuAqg",
        "outputId": "49b6a645-0f5f-4768-86aa-ba4117eabd32"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9997586607933044}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Language detection using pipeline()\n",
        "\n",
        "load a pretrained language detection model and use it to identify the language of a sample text."
      ],
      "metadata": {
        "id": "BfpLnCfHve5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
        "result = classifier(\"Bonjour, comment \u00e7a va?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "52c6120ea1e84e40a8999a0d043f003a",
            "3d81cf21798b4a93bbb9d8792415e586",
            "2b886dd092924d6895c378cc2f724795",
            "0ae5dec2ae1448fdb0b0bacaedb6cfc5",
            "1560330ac2f54bd0afe1592826d4c364",
            "98afe4ba36074ca8a0249638c51498a9",
            "4392b3614ef44d9a959f618c97cd4d4b",
            "c6a0a8427fd64bd893b8e1f7a86b8858",
            "f3e51eecceed4914b825e3fc0d749ecb",
            "4db44684acac449483d97aa73974e529",
            "415659e9072248ca80ea10ab44da9768",
            "204eb834ec734c7fbe78323772c24d8a",
            "bbb5775b18e049dfb8dc160be7920733",
            "19949c1956c54e4ca9bba1a487a9063f",
            "ca3a0e7f47624186880a0d4cdf5f5018",
            "1184f2b116814277954ee6ea5962fafb",
            "8e4168e4d49848fa9291fb731ddd7dfa",
            "c6fae37848174337bc3a807b4d559bda",
            "6bb23ad2d6a849c8ba73f73f1c83d89d",
            "2f1c6ac54dec466d89fcebb99e862f47",
            "120b1b39ec6d4828be62ea1ef9da7951",
            "38007bb39bd74fd8b8e2858b689fc26a",
            "570853b968db4b5b895954e83acc8b47",
            "a49baf82c99740bba8ed8fa6184fc691",
            "2087b58039ab44309d5102b54e04a7aa",
            "d46743e1807a4bf2880084da4b980a51",
            "055ac7f612424b2db66ea57316cfff24",
            "c43a5945fdab49ec9d63c4fbbf047188",
            "258f25114cae4261bd0da518eb32d60f",
            "35f6035308d048948a82b897f13fc910",
            "247fddf19d7446adb136810dc735dedc",
            "4d5ecfdcf9264a5aa6222f57ba398301",
            "2431c580935c4765ad3901dfa859e1da",
            "cb004ff9d8c64f94bc0c4fdee5112584",
            "89ef5ddb13a04ecd8b85d28a31d5c0f3",
            "fe8b4ce85c1c454bae47d3223e3839b9",
            "3dc7e8bdee0145b9ba532bbdf5ce9c47",
            "0dc8c46e54c44df488a18848276a7bce",
            "f28c0a3f5a04492abe4f478c7662085b",
            "7565d8a1f4984bfaa3ad38e394a58664",
            "4445567c07214f13a45653ad35dbb6d5",
            "93f42d902935414ca1d304d69e971995",
            "a5aa6447a3924f3a8b18077676b885bb",
            "ea0f64353c1d4620ace94b0f44595cfc",
            "fb05dc6e92b24cfeb502019b9cb388ee",
            "e2f13f26509a4135baebd856bb674569",
            "b249a8ded6234250a796b2e6bcbd2fc0",
            "a5ddee64f5d14152b13e6abc70e75eaf",
            "48a9d780736e4d76ae9d45ffd70ef6ee",
            "f08b6cd747e74c7590ea590e63abcf42",
            "f942751bcf3347c0a272c0a469fd8ff1",
            "10e0f3a8bece43cb97dc4fa71536060f",
            "f16f5c9da7d24de5b6204979076596a7",
            "3cb5174d85184896a15f739085853495",
            "b2e288fde4914c8d87505a485f723dc6",
            "b162b2e9f4fc430b83c5f86898eafe98",
            "4eeec3e02df943a89b567c4a2816dc93",
            "16ec165eae074655987adb0d024f1ec5",
            "b846962bf54b4a45995c61eafa00ccd1",
            "7fa90c932332474e84e22a89f4489ef9",
            "e7eedcedb3ee46cbaf80c8e40fc050e7",
            "d10f362c75f54d228bbf98b8b5e764a5",
            "e108b5894f0847feb44dd65533454d06",
            "28fcc48ed4cf432ba81f296ff1f29ac9",
            "0198501c22344f0d9c1381353380defc",
            "10c1555a934349c28ce36dc570324f1f"
          ]
        },
        "id": "Yq5BpT8hs1N2",
        "outputId": "5a90892d-cb94-4ff4-b69c-bb00c1faf67f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52c6120ea1e84e40a8999a0d043f003a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "204eb834ec734c7fbe78323772c24d8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "570853b968db4b5b895954e83acc8b47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb004ff9d8c64f94bc0c4fdee5112584"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb05dc6e92b24cfeb502019b9cb388ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b162b2e9f4fc430b83c5f86898eafe98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'fr', 'score': 0.9934879541397095}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text generation using pipeline()\n",
        "\n",
        "load a pretrained text generation model and use it to generate text based on a given prompt."
      ],
      "metadata": {
        "id": "ywykJNk_v1Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text generation pipeline with GPT-2\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FujQ8CGov6OW",
        "outputId": "5f268f65-12f0-46a0-ea4a-cd67ca23ec51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text based on a given prompt\n",
        "prompt = \"Once upon a time\"\n",
        "result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc9y1sPXv-Ud",
        "outputId": "05dd53e8-e8bc-4df2-ecc1-953bfee0cf8f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VhgCCEUwKEK",
        "outputId": "0f979055-0bad-4572-ea4a-ac49c6c3cad2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Once upon a time in the past few decades, many Americans have simply moved onto a new \"political party.\" Republicans are now in charge \u2014 and, in some cases, they won\\'t be coming back for another four more years. Indeed, Republicans have'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the generated text\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLugg9b8wRal",
        "outputId": "1e759bb5-1383-4681-c583-7cfc876a2e56"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in the past few decades, many Americans have simply moved onto a new \"political party.\" Republicans are now in charge \u2014 and, in some cases, they won't be coming back for another four more years. Indeed, Republicans have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text generation using T5 with pipeline()\n",
        "\n",
        "load a pretrained T5 model and use it to translate a sentence from English to French based on a given prompt."
      ],
      "metadata": {
        "id": "cQN1HkEuwb7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text generation pipeline with T5\n",
        "generator = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
        "\n",
        "# Generate text based on a given prompt\n",
        "prompt = \"translate English to French: How are you?\"\n",
        "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Print the generated text\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298,
          "referenced_widgets": [
            "71598adf438c494984bd798352422134",
            "ffb9d6f8e72a440ea359899938dc68f4",
            "ca8437d367fd4371a9b5b02c84e7a90b",
            "527810683a0d4699867b6a2f877273b2",
            "b44ac3eb25a14a74ac662d44cf5c32f5",
            "5fd5608413da4391b749a91c4f6caa8d",
            "51dee06ecf894bbda70a828e499adbeb",
            "ba8064c21760498fb5ab7f7e4fa9ea06",
            "be02a80bb9d74995b893bd6666ad2b98",
            "8b2380e3579f40ef90475c00a03be0fe",
            "9e1a7865a9514327b2c874fa9eec1c8b",
            "dfc07fd1d6284ab5a5169a2f6709e594",
            "e122f50a1ff449cf84d12e68145f5f3a",
            "bc63a75d267f4c42a18b92ce505b6d8d",
            "56c68ac4d1a04b46b12dedee66d2d980",
            "20115af7495a4239ab49f821cab0c62e",
            "081b48392f5343cd9a47225641e87a4e",
            "1ba438700fd84ae2967f7d047d585e03",
            "02df6b7d1fc243189c84f242e4df5ff2",
            "475fe41ad4424525bafd0955b45621e5",
            "3b9fa080444343cf91a355185d8cdb54",
            "dc58f81f5a8b4e18bff5cb04bcaf345d",
            "58bbf2bf805c46d187dc9c5e10d72c0c",
            "ec11ea338ca742b38fc5d0b9e0a996a6",
            "fe24958df7aa40f98cb621e126f32483",
            "6bd7094d9bc0440995e4f3d26569dfa2",
            "2c1172b1e6dd4c6f9a65f9fabc26de3a",
            "21effdb4e84846ec8e532b8c23231a20",
            "033fb1b77d544ef09c683e4a6e71b079",
            "b00891e4ed8d435a9767390492f4ec15",
            "05cc6dd3336a4faaa3a4c312f120d971",
            "a367b8a4f8074294ab2a2ef52cec7f8c",
            "3fa8a8b0cfe942f99e6b1dbcf2d6ee19",
            "24187d6e953a4465987afac432b271fd",
            "4e51354143344942820090b95d7cb003",
            "1804f610657449e981d402ab01188529",
            "deb131b1c9c14c1f812d7c27c10ce4a4",
            "028bee11f4f04445ab8bc7208433a87b",
            "98e11d62c8c2410b986cb71cac165453",
            "f65f171b2e5e4906bf89948f42cb90a8",
            "9cbe3226a4354ae2a0d6b4ccb798aa6b",
            "7e8a7c7fcce244f89ca7f3d1212d7555",
            "6d2c5339771d43248b9ab876912a23f7",
            "561f2e99dcb94eafbba93b505e459867",
            "ded24dda60af4f2d972e90d3dd7cce8b",
            "3655cd8d786148ee9262acf4c645cdcc",
            "c79aaf22e7e84750894fcb9dba1ea3eb",
            "0a68962882f24eff91fa3cd8b15519a4",
            "749edb8c1ad64000a93631e21bc1dc58",
            "d19dc4bea32548a880b99f3b8c769a04",
            "ba468ce2bd9541fc9a9fdcf1c9785dbc",
            "f8a21675d2454c05be8b7e62928045d0",
            "53efa96d87764b1f8d08b9707d4b7f0b",
            "407dae601fe84ab884e3225bcb677a2f",
            "8a9e0efcc7bb4fe5a72afa0c9a4d61ee",
            "00202777cc2d469f92d46adebc8664d6",
            "0f851a54f35e49809ecd4a6edc58d2d7",
            "6e338b3afc83404180643661fdeeb156",
            "f5782b30a17b4880a90b8c088cf9d554",
            "c06537c3b35f44a9b2b1500f442226fc",
            "ada21b170f53463c8aab76194d227305",
            "f8fdb46ac9124789a32446be0ee759d0",
            "039a7cac73d6497b8db07bfb6e176168",
            "d3881e53a70543e4bffbc7db8f97c02d",
            "dec70f81546d42dd887ff753d64bf4ce",
            "370d2739d8a74be9b0b779b3c9ac5fdd"
          ]
        },
        "id": "Zp9Ri21NwUyn",
        "outputId": "5603581b-9995-4635-9d2e-107eb80b8cc8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71598adf438c494984bd798352422134"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfc07fd1d6284ab5a5169a2f6709e594"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58bbf2bf805c46d187dc9c5e10d72c0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24187d6e953a4465987afac432b271fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ded24dda60af4f2d972e90d3dd7cce8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00202777cc2d469f92d46adebc8664d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment \u00eates-vous?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When to use pipeline()\n",
        "\n",
        "- Quick Prototyping: need to quickly prototype an NLP application or experiment with different models.\n",
        "\n",
        "- Simple Tasks: When performing simple or common NLP tasks that are well-supported by the pipeline() function.\n",
        "\n",
        "\n",
        "## When to avoid pipeline()\n",
        "- perform highly customized tasks\n",
        "- need fine-grained control over the model and tokenization process for performance optimization or specific use cases."
      ],
      "metadata": {
        "id": "aa8lHvMYxfNk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQoha_aLw8zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill-mask task using BERT with pipeline()\n",
        "\n",
        "load a pretrained BERT model and use it to predict the masked word in a given sentence."
      ],
      "metadata": {
        "id": "CA_higIHyN1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the fill-mask pipeline with BERT\n",
        "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "# Generate text by filling in the masked token\n",
        "prompt = \"The capital of France is [MASK].\"\n",
        "result = fill_mask(prompt)\n",
        "\n",
        "# Print the generated text\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "2e7777427e974f0b8bcb3ff9a87bcffd",
            "cfa3f4b35e40451faa9c552edab528be",
            "43c8f9ceab9f44309ce3b992b1c21bd3",
            "834818f519e94569b7b04ea3dfd17791",
            "ca62e32b76bf4094a94df4b5e94e3ad9",
            "53498f42776549379a7a7b3dbcadde8d",
            "0345d98ae7f54d7da3f6bb2059d71d65",
            "5ee35962c0f944f48f98482912b38426",
            "8ffa4b992f204e0badd69e19cdc62b64",
            "51465ec9f08d42bca2125d5e21858540",
            "bbb98050cd5940dfbad7c62ce826e0a5",
            "795daede3d3d4f6e8e75fe4bbd21dadc",
            "3afe82d7fcd24dd3bb1cb0bb681ef628",
            "91b772e2861f473fb72685bf2392ffb3",
            "561abb472eb94bb98eaadb2c2f450a09",
            "a2af428c6ac64fe09257b6551aaa4548",
            "d003b1ce28964910a81d61dea3bb1129",
            "a0ded79da29b44adb8466ad1863f0896",
            "70a4a90c12dd477d93ec6084610af528",
            "9847111af243432db9af5b3a35a51584",
            "5bbfe20027334400a4166f0c5969209e",
            "a1f3d694cc64488f92d64bafaac686dd",
            "d79da0cb8912458883406e0854bb1c6c",
            "c0797f47153144a287e37a2d6f6ec9a8",
            "9a0bee74f3b740ca8e181771334ed06e",
            "bbc0357eae0641c58ea67e9a9513e8d2",
            "96820efb29c9414a9f18c072db0a9973",
            "1a748b60f53845f693d60a141023dc0d",
            "1e322626b70e40dea8ecaa10f031e133",
            "a7dffa27dd6e42029971f94418f46d71",
            "66e1b920ddf54c209636c8962a8f8478",
            "19aaeb1c69bf44d1b15a2dd5211c248f",
            "097fb97f39374e2bb1295f32b6e7fc91",
            "8e583098f462415dac9009b7974d26bb",
            "571d646a7ea14b9699bde17e292755fd",
            "0a78b4119eba4a60a8cd097e4d7dfa7b",
            "747d29f62a414d9780359cae10a618b3",
            "8cd324c8b0bf42a1b59441e4c9570108",
            "a19a2c270e824489837509f9d1631650",
            "63cad9c9713244b2ab0e38368633f160",
            "0e8e18ce9c7e4d5697fadd37606d6568",
            "dd5949413f574839a842b1aa1c2b2faa",
            "454c3d2d0cd244798697d8e2b42e1ed9",
            "baa8571ef4d64d289af59cf971ac64db",
            "24f88dc1ef4e46719f1975a75c28a069",
            "484e38439dc846a6bfea4ffddf3a69ea",
            "53c48b39a6114a6e9a7717e4ff3aa744",
            "878ceea89ddc4e97991b5cdf043d13c0",
            "f42eec76a77f46e692473497a0ba91a2",
            "44b8f2c4a7d547478463be9b96c88d7d",
            "162b1159f27647dda0c01fd5c9d00135",
            "e07bc932aee34b71bc1e635867e91655",
            "381c05d9edd8457ebcc38819454d6da7",
            "ace3e168c3534f13a453a6c3663baf8b",
            "402c802ab0cf4261a1cdb3b2d35804f2"
          ]
        },
        "id": "IXVC8X_fyRYI",
        "outputId": "8bc07dd7-c8df-4202-ae25-61252f66887b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e7777427e974f0b8bcb3ff9a87bcffd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "795daede3d3d4f6e8e75fe4bbd21dadc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d79da0cb8912458883406e0854bb1c6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e583098f462415dac9009b7974d26bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24f88dc1ef4e46719f1975a75c28a069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.4167894423007965, 'token': 3000, 'token_str': 'paris', 'sequence': 'the capital of france is paris.'}, {'score': 0.07141634821891785, 'token': 22479, 'token_str': 'lille', 'sequence': 'the capital of france is lille.'}, {'score': 0.06339266151189804, 'token': 10241, 'token_str': 'lyon', 'sequence': 'the capital of france is lyon.'}, {'score': 0.04444744810461998, 'token': 16766, 'token_str': 'marseille', 'sequence': 'the capital of france is marseille.'}, {'score': 0.030297260731458664, 'token': 7562, 'token_str': 'tours', 'sequence': 'the capital of france is tours.'}]\n"
          ]
        }
      ]
    }
  ]
}