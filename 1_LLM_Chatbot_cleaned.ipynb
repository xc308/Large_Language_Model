{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFkyj+ZIMvlACfOCL+GD2i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xc308/Large_Language_Model/blob/main/1_LLM_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a simple chatbot with transformers"
      ],
      "metadata": {
        "id": "UqQKJpaQoUHz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2Ahc-aKoLG8",
        "outputId": "e28372e7-dd2f-4dc9-9d34-ced48f891cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.42.1 in /usr/local/lib/python3.11/dist-packages (4.42.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (0.30.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.1) (2025.1.31)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting torch==2.2.2\n",
            "  Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0\n",
            "    Uninstalling torch-2.6.0:\n",
            "      Successfully uninstalled torch-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\n",
            "Requirement already satisfied: torchtext==0.17.2 in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq tensorflow\n",
        "!pip install transformers==4.42.1 -U\n",
        "!pip install sentencepiece\n",
        "!pip install torch==2.2.2\n",
        "!pip install torchtext==0.17.2\n",
        "!pip install numpy==1.26.4\n",
        "\n",
        "#!pip install --upgrade numpy transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Selecting the model. You will be using \"facebook/blenderbot-400M-distill\" in this example.\n",
        "model_name = \"facebook/blenderbot-400M-distill\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429,
          "referenced_widgets": [
            "7ea94abe74d34e95945996776abb8a28",
            "5c567a4b40ea4cd6a5dca61fa2d6f457",
            "7d294c142a8d42b6994b6fbd47b92803",
            "79ab8f844eae43f8906d911c8d2691be",
            "1f57c4c839f34c71a3f6c180d862222a",
            "6f5c249d1d0149e1a205162e4fdc4b1f",
            "d4d815965bbc4221962276ab3e9f7dc1",
            "931f3df49d984bc5a9ccc19f1b677944",
            "289a8956cd944d45a2009ef8eeb2566f",
            "40691a38415c4515bd3b239b8839f837",
            "e36aa7385b1b4dbb82ba945847fdb9be",
            "4d3ceb4b8ef84acd80e6032a88e8d108",
            "d5408af9640e44c2a4afa275021d2e17",
            "d7df8e3e07ec4223a8f325c868955d81",
            "1c3d55a993da448692ae435c8ad162fc",
            "ef0cf113013a44cabdbbc303fa81bee8",
            "d0a5d3b30c5642ff8d141d570ff48988",
            "e704396d7c3244abb54928d098e7dcc5",
            "d57af32006f44d95aecb9478b230432a",
            "4179669d54ba48f8b4bac9700cf23e17",
            "bc033cadd05f4dc5a4f56e2024a6b1e6",
            "6a7ecdc96367455abb38d483acecacb0",
            "e1395f80abfc4cb78a19d71d2a1a76ed",
            "8ec488f59c2e4b81b166118986900c75",
            "737fc9fb94944563a813b717ccc4ef96",
            "8a9b1ea5c1284c1ea41bdfa16fe9a8b7",
            "ecc6d6275af84dbbbb0be7f773374ffd",
            "6f8d2997c68f4728ace65f3044650a3d",
            "0ac21408754d4e92bdc9f1dcc5d5ba74",
            "87212ceee9a8495f80faed4173f6154a",
            "88571eb828124fc1834f7f3f6f219443",
            "30db04a395744de8b0830b7f19ddae35",
            "c7f61bc9602544d48675a28f7aabda6e",
            "d3b3057a41a04ab3aa833a276d0860c0",
            "3fbb560266f744339c18dd880b7a74c8",
            "a62c2a7ddc65490e912a2420b278481f",
            "1f60cf56bbf54a59981c601fa9e18ab2",
            "a6311584c07a4d70975d7bc8eaacd50a",
            "47be26e7786a47dfbd246fd53bfa6849",
            "471253db321f4e5187c22edf0919d7e3",
            "3fdb120de1b04965b2d0a506550838db",
            "24087be4b0684890b83359fa76665236",
            "dde3cd77d6c2457786640d16f5f50a8e",
            "b8a4edc51076432ab2febed77e018261",
            "c44b32c93b6c48e0a6a0e7594e69e47b",
            "55ce636f301f4d419f9339bda7efad9f",
            "ca73cd72e21f413db489adf06117a73b",
            "f0625a9a411d48d08fcb829b382d528e",
            "f4518428ad9143eeb13dda1a7083564f",
            "90678adf31da40afb0103bcc50b4190e",
            "65701fb15f794b2c815a401191273367",
            "8a8e07ead9a5473e8f9ba8a1c976151b",
            "27a3f6a806ab47268ab171141b3a2f68",
            "dc835cb965304e739cddee6668505809",
            "ce6f2e2d2f32443fab3ff15e6827ba75",
            "9ca63a87a2db460e8121b38d3c29ab08",
            "da6ab633d7fb4cc6b1ed23115848269b",
            "1358357f31344379b32bf07e50d19c5d",
            "9d0a1d0b3100488a86d38573d5793a6f",
            "9fe0dc5044fc429f910ca95011f6f4e9",
            "2d4217fa7e5c41f48d5625b4a37944c1",
            "4fbca45bebbf45e094d80d5b9bb662f4",
            "0f82714fd5dc4f529309cd4c12b8fc3b",
            "1b596540f6714baf8be1a1f2c85b60e1",
            "bf284ea2ed2846259d7b908afa1464fb",
            "df6fb43eec1e4848aa08dfc08b528ceb",
            "57cc126c5b564f19b4b7018e3ea61998",
            "20bc5416a67e410581108aba8cc9f7e7",
            "1c75d848d5d84b4e8f3da6f3bb74c41a",
            "28d8142c9a9945fc8e9a4a75e9984f36",
            "d8bcae41bd324c21ae67bce6f501658b",
            "9e5b7b82a89f40a6a483d5e5d9b604ae",
            "ba3a627556d748369e1c810bf0457992",
            "cdd32166dded4861b808116c126d98f4",
            "6120110b2769436ca92ff87b6f50f194",
            "ea5266b91d0e436cbf4d1865a566357a",
            "34bcf893a61c41bd9ebd5781c73af21c",
            "a8f30c6d52ac4cb5aee750dc3ffeb67d",
            "20eb2882c49343c8acd711524a983eb8",
            "31441157fb334547b5f4b4b552698bcc",
            "570cae4e5df84c2caa04a916d306aafc",
            "2ec86c99f80a43e88b7cb907348fe32a",
            "8c2e806c3d8f4e18b5c3ce270465fe89",
            "01bfa3f10a0e49f59362fd57bd9691ed",
            "055e75e5ab044daf971003d0330a8814",
            "23b0a1fd7fe24c29bdd5755476aca622",
            "13ed6c7ae6c44f55903fdbb14488c751",
            "a51bab58f3544084a8ed752c70ac6287",
            "d1a29fa6005c4ffa86cc53a74212ea90",
            "c2b56d3d3f0844088cb19ac1ea8a6f95",
            "81d0964018874aa09715a1a3d9488fea",
            "06fefa73357d421492f3392257ccc25a",
            "7a44ac4db9604078b3075a274bdd8f39",
            "96b65fa8e1b040829daaba1e9bef750a",
            "5b1edda10cc742bd9499a2115821457d",
            "4a322a0ec9ea46e4a5c0edd8d0f7b15b",
            "67ef7ca4e5a9402e82a46d3f02df9e6e",
            "782fbd3e04ed4448b3addc4a561ddbf9",
            "4539d0bde5834c9f8bbe7b26f5ee5e5c"
          ]
        },
        "id": "imNzhtNxtMPq",
        "outputId": "bff1bf49-43a2-45be-a480-d989676cb3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ea94abe74d34e95945996776abb8a28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/730M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d3ceb4b8ef84acd80e6032a88e8d108"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1395f80abfc4cb78a19d71d2a1a76ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3b3057a41a04ab3aa833a276d0860c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/127k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c44b32c93b6c48e0a6a0e7594e69e47b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/62.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ca63a87a2db460e8121b38d3c29ab08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57cc126c5b564f19b4b7018e3ea61998"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8f30c6d52ac4cb5aee750dc3ffeb67d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/310k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1a29fa6005c4ffa86cc53a74212ea90"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chat function\n",
        "def chat_with_bot():\n",
        "    while True:\n",
        "        # Get user input\n",
        "        input_text = input(\"You: \")\n",
        "\n",
        "        # Exit conditions\n",
        "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Tokenize input and generate response\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\") # pt: torch.Tensor\n",
        "        outputs = model.generate(inputs, max_new_tokens=150)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # Display bot's response\n",
        "        print(\"Chatbot:\", response)\n",
        "\n",
        "# Start chatting\n",
        "chat_with_bot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRXGIX17vjnU",
        "outputId": "b4923a40-91b2-46f7-c151-90b4ed421100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hi\n",
            "Chatbot: hi, i am a little tired from work\n",
            "You: what did you do for the whole day then?\n",
            "Chatbot: I went to the gym and ate a lot of food. I was so tired from the gym.\n",
            "You: great! What did you eat today\n",
            "Chatbot: I ate a lot of burgers and fries. I'm not sure what I'm eating today.\n",
            "You: tell me what is Transfomer\n",
            "Chatbot: Transfomer is a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for a slang term for slang\n",
            "You: you are saying nothing!\n",
            "Chatbot: I'm not saying anything, I'm saying that I'm not a fan of the shit out of this subreddit.\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying another language model and comparing the output\n",
        "\n",
        "\"flan-t5-base\" model from Google, to create a similar chatbot"
      ],
      "metadata": {
        "id": "mA2J6eE9w_oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Selecting the model. You will be using \"facebook/blenderbot-400M-distill\" in this example.\n",
        "model_name = \"microsoft/GODEL-v1_1-base-seq2seq\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "412262ea28e74390b18340b6ca0a6fe9",
            "07bbd28782e2453b8a7a93d0782feea8",
            "fd4d37f255d84f9c86e4e8f4a1de09c4",
            "b12e8c7baa284e06a19124035d018855",
            "76ecd373c64a49f8827a1a742ae25d9b",
            "52637adf86864c83a8aaa586073b1dd1",
            "8b868b778ee546e889ace3ac17c68ebd",
            "7dfd9273e1e14adc83c69694b85942a9",
            "3d60b968726243c59e3ae54cb9f06900",
            "f8a14b90233d4705b63dea36cbd74fdd",
            "45e547a1e26b48059a43bd1886c2f42a",
            "82061626caf24e91821f464c4ee5429b",
            "907a9e44d60341df87ff74853d3df782",
            "fc5e9619c487468d813acd3fb9c39e23",
            "96c202a00cad46649076c66216f45c4c",
            "5fa5400e273c4caab96e0909e5e0f39a",
            "a9530c15452043ae92ad14794159259d",
            "d35db8347bc345b28224d9124159945e",
            "3060cd478edb430985b77addc54f5e52",
            "1201b5e93acf473bb961c64d0baab5b3",
            "57f2bbd60b554fbb9de6a9ab7074175d",
            "861736d8314347baa489cd3400bb9ce1",
            "2924cd1c64b34e3e8ca64a97c5ed67da",
            "28bcf2e4854a4325b72cf9d421c17c10",
            "8a57370cc817453bb066a69ff9018745",
            "220460ce514c42fbafc26184e3a5039d",
            "5927b954ac3c4281a25ec2e510ac4274",
            "8f7ad03179214cb59b3e3d47a53a3dee",
            "a098f782f5114f0ea91117402d1c0de9",
            "2fbc2b0f57b441d2b485d9d94ac09517",
            "6e04c0ff7f5a4c8ea7f8aff52db2b6c0",
            "c332c9c55f0c440e8ba5da65e3f15be4",
            "0abd19e16ef4476393556d32cab48a34",
            "62fd49cee1424280ab149f55aa7928fc",
            "2b4ba74c07b24269baf29737af98f58c",
            "070ce05e57cc47afaf0f51a7aa50a1fd",
            "31caf2236e4347259815584ed574739a",
            "8f13e122e14a4143bbb00a11db339849",
            "22351ab01841463381acb4e32dc22cdf",
            "61901ee941cf4f2fa20ba5a5e22989f8",
            "b8d3a26ae5e9466b86b9d277c96b5055",
            "0b5fc6bacf664c4e82a9401b8e98eaae",
            "73da23886b61419eb7692a79cb7053e8",
            "11406d50e8c14387bac19ce3d4c21dba",
            "c0d362a6e10447f3be3eab51d2a7639d",
            "78b34678aa014714b63f4513a2cd21e3",
            "eefa97442aca487d8f955a6daab73684",
            "d0ca4bfdb62d4bac8baf9b20aca6d749",
            "4aa9bb8881204dc29d649b7b9a720a7a",
            "b890e9017f434a49b405a68e9388a1b9",
            "bda0f1c266134cd3aa528da4e8466b24",
            "7cd6a0a0e2354768b74afd47a4c12f03",
            "0689d962768c4ed589cde28c833fe167",
            "4cb7c908474b44e481b8cd443d9296d2",
            "71c3c24197b04f179f9497ced7221c72",
            "78f01f8fc3344bd5837997bff67d6569",
            "c46e859700dd459cb7a138a382debb02",
            "e96e5ef4b34840919c852a21beddd262",
            "a7bad50caf8f4b7c8a349b914dfcfab3",
            "67c0f54919f4486ab6f78d991ebec5be",
            "333d0b9420794923a4c481ccebaadd2a",
            "84f37dd4e78341dd8f6ee635bbe76af9",
            "f8e90c0c502a41439b76b997f11797e7",
            "57d5af1c73fd40bf9244aa3b0f5e1da3",
            "9689dd9a4c9b4d2cbc5c6160a217558c",
            "8ec6182481584693b2b4634ebd3551c5",
            "449e9afb0e2c426aac626b80a8cde790",
            "58eaead516bb4e3c948c4d532928d6eb",
            "bed42a1d78ff48a39c428c5fe328b773",
            "803abdae3fa049879b810e8020de7896",
            "307cfb75fb4a4192beb53b5ac2c3a36b",
            "d6fb668d407a4c04a30aae7700648fd5",
            "459bf2a481474a9ea025ca8978da3df2",
            "b4017a486d144533b4a4d0b8e5c596ae",
            "cc83d9e9743141f49d9403b16adff232",
            "8d7a10fd868b4a79b0b862a722adf816",
            "f0b12d25e0644dc0a5056bfd51571d2f"
          ]
        },
        "id": "vybs2Ir33TWT",
        "outputId": "8c65c695-190b-4082-8c68-8c3f3af9f229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "412262ea28e74390b18340b6ca0a6fe9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82061626caf24e91821f464c4ee5429b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2924cd1c64b34e3e8ca64a97c5ed67da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62fd49cee1424280ab149f55aa7928fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0d362a6e10447f3be3eab51d2a7639d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/37.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78f01f8fc3344bd5837997bff67d6569"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "449e9afb0e2c426aac626b80a8cde790"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chat function\n",
        "def chat_with_bot():\n",
        "    while True:\n",
        "        # Get user input\n",
        "        input_text = input(\"You: \")\n",
        "\n",
        "        # Exit conditions\n",
        "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Tokenize input and generate response\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\") # pt: torch.Tensor\n",
        "        outputs = model.generate(inputs, max_new_tokens=150)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # Display bot's response\n",
        "        print(\"Chatbot:\", response)\n",
        "\n",
        "# Start chatting\n",
        "chat_with_bot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfc11Jri3mHY",
        "outputId": "4f640467-9319-4686-bf9e-e97a3af1a00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hello\n",
            "Chatbot: hello, i am a little tired from work\n",
            "You: me as well\n",
            "Chatbot: I'm a little tired from work.\n",
            "You: Ok, so you don't want to talk with me now\n",
            "Chatbot: I'm sorry, I'm not a fan of you.\n",
            "You: what?\n",
            "Chatbot: I'm not sure what that means.\n",
            "You: what time is it now\n",
            "Chatbot: It's at 9:30 am\n",
            "You: where are you\n",
            "Chatbot: I'm in the middle of the street\n",
            "You: What is the capital of UK?\n",
            "Chatbot: The capital of UK is London.\n",
            "You: what about France\n",
            "Chatbot: France is a country of a lot of people.\n",
            "You: I mean its capital\n",
            "Chatbot: I'm not sure what capital is that.\n",
            "You: You don't know what capital city of France\n",
            "Chatbot: France is the capital of France\n",
            "You: I think it's paris\n",
            "Chatbot: It's Paris, but it's not a city.\n",
            "You: you are not very smart\n",
            "Chatbot: I'm not smart, but I'm not a smart person.\n",
            "You: Bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "76f47d8af8f74edabc00b7a257264339",
            "881409d6841949e4b9ada34ea960aa30",
            "95efaa13532341859cbedd43951e2447",
            "aa7094a672c9487a8ef75563670a0ba4",
            "60446aeb600342eab25180ec74dcf9e7",
            "974a515d250b4f1682674cb6ff65127d",
            "b275912093e1490c8449a9da085f3b32",
            "3577935890b94b21b21d07c058f29873",
            "082b49506d744ec7bb006911b22d31e4",
            "d067c06579fc43aa8e65208947986016",
            "7f3639ab9a1b444592f0562b671d24d8",
            "2f24ce35062a4db3ab1a1235d37a1a63",
            "21634bb65bd5418ca8faf11004b6513e",
            "c8414397a5954d9eaef86ab85dcde40a",
            "981fee40aee34db4a38ef43dd7b0bc6a",
            "f458ca4874f04b75bddd88a30244c6ec",
            "8ea3500560114f539ad97e666dd9461c",
            "19717e1a2b7f41c496e333f5143c351e",
            "5a518e09804f42ee8ced36d2353c030f",
            "63720e9d098b4ed4be4b4d35bf41f184",
            "bc7232a47ede4dff91e2596b6f34039a",
            "295f5c691b2b4b6990c82cb0e91f230b",
            "a5e344cb47244175a23e4232cd18e110",
            "5bfc196e27564bc3b080d26c66a2c55f",
            "db6884e3fc054790a2ceb2529197e099",
            "5f60a718dad640898d3c3ad6b89ad9ed",
            "35aa3c54aad14d40875957cd740f7b60",
            "000e1423287541cab85fbc17985cb82c",
            "609eb23c4eab4422aaadaa61b9983b82",
            "57e75408ce7e48f4a8be45dad5c2fb50",
            "752e0dd98edd41c4a8c29607d0d4a3dc",
            "605006dbc5aa4410ae917734219c9025",
            "be10a0cec2964bf9aee9484320c4f63a",
            "03da0a0454c14fe9a164cfcca5391673",
            "83cb5a93a5f6440ea4c19f5e1a86d5bf",
            "4aac5a8c1cb842bd95b9f09221d93b18",
            "e14fbde93a9e449bae3e6024de21c09c",
            "20c22e613d0b4b24a90eec5b1598f456",
            "f47fd4ef03434429a6a9f8ac2d489346",
            "07a4822945ae4cfcb36f78ff8928eee9",
            "59d0d875f0ea462987cab12043d22e16",
            "e00b94a4570f42b1bfa14f2abcf0b51f",
            "5a85a6c33a1c4c2da6845c412cb7839a",
            "7ab642d0458247bf9f847fe02c8acb1f",
            "751b860339ea42e4b9dda1e3ccbf6c90",
            "0298b4970a00404daff3715dfd98a99d",
            "73eb5d7feeac40f9a23dbb1b56ac98cb",
            "cfe70e82d6634c77a18123ca3b4a01f0",
            "9c9deecfc2ec42f8bc33362bebedca10",
            "0e3f434bae834c86b1c4fd2855794091",
            "3645c50b7dc64423ac04b7244c68f6d3",
            "332aabc14ee44c93ae73295d8671b5bc",
            "c6f3089e2c52498e8e7f7ba5c2cd10db",
            "98e22c7b43f14115b3dacf68f068f5a9",
            "8578b8b034284141b0f7213c8e1abc92",
            "8440dddf65c749de9ddafdd49e120db9",
            "dbe54efed0574475bc4b345036a2f966",
            "bd4dae185a814ee8b51158186e365e90",
            "920ead6832b04c838384245ab04a6823",
            "50f7d0de2e7d4368bb8ffa9af0b2775b",
            "d94ed256f50c4d6388f6a7fc7a8d5e8f",
            "921124cc6e2f4e98902c78165cd410c1",
            "e4ab830418544536891a36fe652ac580",
            "c69da92f43f44d86aba85b83813a7c3d",
            "ffed9774a7d84892b21bc8b4d776784e",
            "7d42ce62e916431285773a21df2511e8",
            "1ef279231a934831b9f02dd1c63197b9",
            "90acafb4741b4503ab4c5b42224e0e41",
            "a4fbd655b77b44a5a989873c7048b84f",
            "7abf0657cc434a118828233cc5f9b055",
            "6b078fba88c6430486e068892dac1e90",
            "1521bc92c6f14c5ea273201ae83a1087",
            "778cdcaf97594e2fb0682bb3472a4758",
            "fb261eee19904ea5b25179bbf0f905b7",
            "22a49c672a734968a124eda52cdf33f1",
            "90fdf202cd864f89b3d27ae666e19969",
            "32d57c8e7bad42299b8c08fcc34e5cf0"
          ]
        },
        "id": "4d-KRlvpw5n0",
        "outputId": "3345a596-2b74-480d-c77a-9c6027084757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76f47d8af8f74edabc00b7a257264339"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f24ce35062a4db3ab1a1235d37a1a63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5e344cb47244175a23e4232cd18e110"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03da0a0454c14fe9a164cfcca5391673"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "751b860339ea42e4b9dda1e3ccbf6c90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8440dddf65c749de9ddafdd49e120db9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ef279231a934831b9f02dd1c63197b9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chat function\n",
        "def chat_with_bot():\n",
        "    while True:\n",
        "        # Get user input\n",
        "        input_text = input(\"You: \")\n",
        "\n",
        "        # Exit conditions\n",
        "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Tokenize input and generate response\n",
        "        prompt = f\"Respond like a helpful assistant: {input_text}\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\") # pt: torch.Tensor\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # Display bot's response\n",
        "        print(\"Chatbot:\", response)\n",
        "\n",
        "# Start chatting\n",
        "chat_with_bot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ksa9_tU07Wm",
        "outputId": "7a38873f-31f0-4373-928f-4b0bdb47c162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hello\n",
            "Chatbot: <extra_id_0>\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54R0PkYIyf1i",
        "outputId": "2bd73a54-28f9-49c2-f214-626fbbb62e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.19.1)\n",
            "Collecting tokenizers\n",
            "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n",
            "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Installing collected packages: tokenizers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.42.1 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.21.1\n"
          ]
        }
      ]
    }
  ]
}